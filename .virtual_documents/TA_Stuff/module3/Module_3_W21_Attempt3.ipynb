import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from tensorflow.keras.layers.experimental import preprocessing

from  IPython import display
from matplotlib import pyplot as plt

import numpy as np

import pathlib
import shutil
import tempfile


dataset = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')


dataset = dataset[dataset.price < 2000000]
#dataset = dataset.drop()


dataset.head()


dataset['zipcode'] = dataset.zipcode.astype('str')


dataset = pd.get_dummies(dataset)


train_dataset = dataset.sample(frac=0.8, random_state=0)
test_dataset = dataset.drop(train_dataset.index)


BATCH_SIZE = 500
STEPS_PER_EPOCH = train_dataset//BATCH_SIZE


train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('price')
test_labels = test_features.pop('price')


normalizer = preprocessing.Normalization()


normalizer.adapt(np.array(train_features))


def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.ylim([0, 250000])
  plt.xlabel('Epoch')
  plt.ylabel('Error')
  plt.legend()
  plt.grid(True)


def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(512, kernel_regularizer=regularizers.l2(0.01), activation='relu'),
      layers.Dense(512, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  0.001,
  decay_steps=STEPS_PER_EPOCH*1000,
  decay_rate=1,
  staircase=False)

def get_optimizer():
  return tf.keras.optimizers.Adam(lr_schedule)

def get_callbacks(name):
  return [
    tfdocs.modeling.EpochDots(),
    tf.keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=200),
    tf.keras.callbacks.TensorBoard(logdir/name),
  ]

def compile_and_fit(model, name, optimizer=None, max_epochs=10000):
  if optimizer is None:
    optimizer = get_optimizer()
  model.compile(optimizer=optimizer,
                loss='mean_absolute_error',
                metrics=[
                  'mean_absolute_error',
                  'accuracy'])


dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()


get_ipython().run_cell_magic("time", "", """history = dnn_model.fit(
    train_features, train_labels,
    validation_split=0.2,
    verbose=0, epochs=10)""")


plot_loss(history)


test_predictions = dnn_model.predict(test_features).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values')
plt.ylabel('Predictions')
lims = [0, 3000000]
plt.xlim(lims)
plt.ylim(lims)
#_ = plt.plot(lims, lims)





dnn_model.evaluate(test_features, test_labels)


def root_mean_squared_error(y_true,y_pred):
        y_true = float(y_true)
        y_pred = float(y_pred)
        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))


df = pd.DataFrame({'predictions': test_predictions, 'actual': test_labels})
df['dif'] = df['actual']

df.dif = (df.predictions - df.actual) ** 2 / len(df)
print((df.dif.sum()) ** 0.5)
print((df.predictions[10] - df.actual[10]) ** 2)
df.head()


model = keras.Sequential([
    normalizer,
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.01), activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1)
])
