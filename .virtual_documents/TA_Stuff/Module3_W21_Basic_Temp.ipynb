# Load some test data
import pandas as pd
dat = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')
#dat = dat[dat.price < 2000000]
dat.head()


dat['date'] = pd.to_datetime(dat['date'])
dat['year'] = dat['date'].apply(lambda x: x.year)
dat['day'] = dat['date'].apply(lambda x: x.day)
dat['month'] = dat['date'].apply(lambda x: x.month)


dat.head()


encoded = pd.get_dummies(dat)


# Import the libraries we need
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split


# Get our target variable and features and split them into test and train datasets

X = dat.drop(['price', 'id', 'date', 'lat', 'long'], axis=1)
y = dat['price']

X_test, X_train, y_test, y_train = train_test_split(X, y, test_size=0.25)


# Create the model and train it, use default hyperparameters for now
model = XGBRegressor()
history = model.fit(X_train, y_train)


# Get predictions for our test data
predictions = model.predict(X_test)
predictions


# Compute the Root Mean Squared Error of the predictions
from sklearn.metrics import mean_squared_error

result = mean_squared_error(y_test, predictions, squared=False)
result

# Looks like we're off by just 4.5 dat on average
# Not bad for a quick run with no real preprocessing


import matplotlib.pyplot as plt
test_predictions = model.predict(X_test).flatten()

a = plt.axes(aspect='equal')
plt.scatter(y_test, test_predictions)
plt.xlabel('True Values')
plt.ylabel('Predictions')
lims = [0, 7500000]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)


housing_drop = dat.drop(['zipcode', 'price', 'id', 'date'], axis=1)
housing_drop['zipcode'] = dat['zipcode']


housing_drop.head()


#from sklearn.metrics import r2_score
#r2_score()


dat_high = dat[dat.price > 4000000]
dat_high.head()
dat_low = dat[dat.price < 4000000]
pd.set_option('display.max_rows', 5000)
print(dat.price.max() - dat.price.min())
dat.price.value_counts().sort_index()


#print(model.feature_importances_)
#print(model.get_booster().feature_names)

df = pd.DataFrame({'names': model.get_booster().feature_names, 'scores': model.feature_importances_})
df.head(20)


plt.bar(range(len(model.feature_importances_)), model.feature_importances_)
plt.show()


from xgboost import plot_importance
# plot feature importance
plot_importance(model)
plt.show()
