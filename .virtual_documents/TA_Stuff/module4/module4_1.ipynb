import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from tensorflow.keras.layers.experimental import preprocessing

from IPython import display

import numpy as np

import pathlib
import shutil
import tempfile


bikes = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')
bikes['total'] = bikes.casual + bikes.registered
bikes['month'] = bikes.dteday.apply(lambda x: int(x[:x.find('/')]))
bikes['day'] = bikes.dteday.apply(lambda x: int(x[x.find('/') + 1: x.find('/', x.find('/') + 1)]))
bikes['year'] = bikes.dteday.apply(lambda x: int(x[x.find('/', x.find('/') + 1) + 1:]))
bikes['week_of_month'] = bikes.day.apply(lambda x: (x // 7 + 1))
bikes.drop(labels=['dteday', 'casual', 'registered', 'feels_like_c'], axis=1, inplace=True)
bikes.head(25)


train_dataset = bikes.sample(frac=0.8, random_state=0)
test_dataset = bikes.drop(train_dataset.index)

train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('total')
test_labels = test_features.pop('total')


normalizer = preprocessing.Normalization()


normalizer.adapt(np.array(train_features))


def plot_loss(history):
    plt.plot(history.history['loss'], label='loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.ylim([0, 100])
    plt.xlabel('Epoch')
    plt.ylabel('Error')
    plt.legend()
    plt.grid(True)


def build_and_compile_model(norm):
    model = keras.Sequential([
        normalizer,
        layers.Dense(480, activation='relu'),
        layers.Dropout(0.25),
        layers.Dense(480, activation='relu'),
        layers.Dropout(0.25),
        layers.Dense(1)
    ])

    model.compile(loss='mean_absolute_error',
                  optimizer=tf.keras.optimizers.Adam(0.01))

    return model


def get_optimizer():
    return tf.keras.optimizers.Adam(lr_schedule)


def get_callbacks(name):
    return [
        tfdocs.modeling.EpochDots(),
        tf.keras.callbacks.EarlyStopping(monitor='huber', patience=200),
        tf.keras.callbacks.TensorBoard(logdir/name),
    ]


def compile_and_fit(model, name, optimizer=None, max_epochs=10000):
    if optimizer is None:
        optimizer = get_optimizer()
    model.compile(optimizer=optimizer,
                  loss='huber',
                  metrics=['mean_absolute_error', 'mean_squared_error'])


dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()


get_ipython().run_cell_magic("time", "", """history = dnn_model.fit(
    train_features, train_labels,
    validation_split=0.2,
    verbose=0, epochs=40)""")


totalPredicted = 0
bikes['totalPredicted'] = 0
bikes['totalPredictedDay'] = 0

for i in range(1, len(bikes)):
    prevRow = bikes.iloc[i - 1]
    row = bikes.iloc[i]
    if row.day == prevRow.day and row.month == prevRow.month and row.year == prevRow.year:
        totalPredicted += dnn_model.predict(row)
    else:
        prevRow.totalPredictedDay = totalPredicted
        totalPredicted = 0


bikes.head()





plot_loss(history)


test_predictions = dnn_model.predict(test_features).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values')
plt.ylabel('Predictions')
lims = [0, 1000]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)


dnn_model.evaluate(test_features, test_labels)


import kerastuner as kt


def model_builder(hp):
    model = keras.Sequential()
    model.add(normalizer)

    # Tune the number of units in the first Dense layer
    # Choose an optimal value between 32-512
    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
    model.add(keras.layers.Dense(units=hp_units, activation='relu'))
    model.add(keras.layers.Dense(units=hp_units, activation='relu'))
    model.add(keras.layers.Dense(1))

    # Tune the learning rate for the optimizer
    # Choose an optimal value from 0.01, 0.001, or 0.0001
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                loss='huber',
                metrics=['mean_absolute_error'])

    return model


tuner = kt.Hyperband(model_builder,
                     objective='val_loss',
                     max_epochs=100,
                     factor=3,
                     directory='my_dir',
                     project_name='intro_to_kt')


tuner.search(train_features, train_labels, epochs=100, validation_split=0.2)

# Get the optimal hyperparameters
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")


'''
msle = (math.log((y_true + 1) / (y_pred + 1))) ** 2
(msle ** 0.5) = (math.log((y_true + 1) / (y_pred + 1)))
10 ** (msle ** 0.5) = (y_true + 1) / (y_pred + 1)
1 / (10 ** (msle ** 0.5)) = (y_pred + 1) / (y_true + 1)
(y_true + 1) / (10 ** (msle ** 0.5)) = (y_pred + 1)
(y_true + 1) / (10 ** (msle ** 0.5)) - 1 = y_pred
e = 2.71828182845904523536028747135266249775724709369995
'''

import math


def MSLE(y_true, y_pred, base=2.7182818284590452353602874713526624977572):
    """Compute MSLE"""
    return (math.log((y_true + 1) / (y_pred + 1), base)) ** 2


def inverseMSLE(msle, y_true, base=2.7182818284590452353602874713526624977572):
    """Computes the inverse of MSLE"""
    return f'{((y_true + 1) / (base ** (1 * msle ** 0.5))) - y_true - 1} or {((y_true + 1) / (base ** (-1 * msle ** 0.5))) - y_true - 1}'

def ypred(msle, y_true, base=2.7182818284590452353602874713526624977572):
    return ((y_true + 1) / (base ** (msle ** 0.5))) - 1


from sklearn.metrics import mean_squared_log_error as sklsmsle

print(f'The MSLE is: {MSLE(20, 13.225806451612902)}')
print(f'The SKLMSLE is: {sklsmsle([20], [13.225806451612902])}')
print(f'y_pred is: {ypred(0.15168280454876354, 20)}')
print(f'The inverse of the MSLE is: {inverseMSLE(MSLE(20, 30), 20)}')
