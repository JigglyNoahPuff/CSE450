# Note: After you run this cell, the training and test data will be available in
# the file browser. (Click the folder icon on the left to view it)
#
# If you don't see the data after the cell completes, click the refresh button
# in the file browser (folder icon with circular arrow)

# First, let's download and unzip the data
get_ipython().getoutput("echo "Downloading files..."")
get_ipython().getoutput("wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/training1.zip")
get_ipython().getoutput("wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/training2.zip")
get_ipython().getoutput("wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/test.zip")

get_ipython().getoutput("echo "Unzipping files..."")

get_ipython().getoutput("7z x training1.zip")
get_ipython().getoutput("7z x  training2.zip")
get_ipython().getoutput("7z x  test.zip")

# Combine the two traning directories
get_ipython().getoutput("echo "Mergining training data..."")
get_ipython().getoutput("mkdir /training")
get_ipython().getoutput("move /training1/* /training")
get_ipython().getoutput("move /training2/* /training")

# Cleanup
get_ipython().getoutput("echo "Cleaning up..."")
get_ipython().getoutput("del training1")
get_ipython().getoutput("del training2")
get_ipython().getoutput("del training1.zip")
get_ipython().getoutput("del training2.zip")
get_ipython().getoutput("del test.zip")

get_ipython().getoutput("echo "Data ready."")


# Import libraries
import pandas as pd
import tensorflow as tf
from tensorflow import keras

import seaborn as sns



# Create an image training dataset
from tensorflow.keras.preprocessing import image_dataset_from_directory

# We're using keras' image_dataset_from_directory method to load our image data.
# See (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) for details
#
# A couple of things to note:
# 1. We're specifying a number for the seed, so we'll always get the same shuffle and split of our images.
# 2. Class names are inferred automatically from the image subdirectory names.
# 3. We're splitting the training data into 80% training, 20% validation. 


training_dir = '/content/training/'
image_size = (100, 100)

# Split up the training data images into training and validations sets
#training_data = image_dataset_from_directory(training_dir, validation_split=.2, subset='training', seed=42, image_size=image_size)
#validation_data = image_dataset_from_directory(training_dir, validation_split=.2, subset='validation', seed=42, image_size=image_size)


#### Processing the data 
# Split up the training data images into training and validations sets 
## But I added batch sizes -- maybe I should make it larger? -- Tipical sizes are 1, 32, 64, and 128 -- is the batcch size even needed?
training_data = image_dataset_from_directory(training_dir, 
                                             validation_split=.2, 
                                             subset='training', 
                                             seed=42, 
                                             image_size=image_size)
                                             #batch_size = 128)


## Val data
validation_data = image_dataset_from_directory(training_dir, 
                                               validation_split=.2, 
                                               subset='validation', 
                                               seed=42, 
                                               image_size=image_size)
                                               #batch_size = 128)



import matplotlib.pyplot as plt

# View first 9 images and their class labels
plt.figure(figsize=(10, 10))
for images, labels in training_data.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(training_data.class_names[labels[i]])
    plt.axis("off")


class_names = training_data.class_names

new_class_names = ['20_KMH', '30_KMH', '50_KMH', '60_KMH', '70_KMH', '80_KMH', 'End_80_KMH', '100_KMH', '120_KMH', 'No_Passing', 'No_Truck_Passing',
                       'Crossroad_Ahead', 'Priortity_Road', 'Yield', 'Stop', 'Vehicles_Prohibited', 'Trucks_Prohibted', 'Entry_Prohibited', 'Danger!',
                       'Single_Curve_Left', 'Single_Curve_Right', 'Double_Curve', 'Rough_Road', 'Slippery_Road', 'Road_Narrows','Construction',
                       'Singal_Lights_Ahead', 'Pedestrians', 'Children_and_Minors','Bikes_and_Cyclists', 'Icy_Road', 'Wild_Animal_Crossing',
                       'End_of_Restriction', 'Mandatory_Dir_Travel_Right', 'Mandatory_Dir_Travel_Left', 'Mandatory_Dir_Travel_Straght',
                       'Mandatory_Dir_Travel_Straight_Right', 'Mandatory_Dir_Travel_Straight_Left', 'Mandatory_Dir_Right_Down',
                       'Mandatory_Dir_Left_Down', 'Traffic_Circle', 'End_No_Parking_Cars', 'End_No_Parking_Trucks_Cars'
                  ]


## Checking the demetions and shape
for image_batch, labels_batch in training_data:
  print(image_batch.shape)
  print(labels_batch.shape)
  break



## Confifureing dataset for performance
AUTOTUNE = tf.data.experimental.AUTOTUNE

train_ds = training_data.cache().shuffle(1500).prefetch(buffer_size = AUTOTUNE)
val_ds = validation_data.cache().prefetch(buffer_size = AUTOTUNE)


## Standarsize the data
### Giving a range based off of RGB channels 
from tensorflow.keras import layers
normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)


## More standarization 
import numpy as np
normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
# Notice the pixels values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))



## Creating a model 

import tensorflow as tf

from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

num_classes = 43

model = models.Sequential([
  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(100, 100, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])


## Compile the Model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


## Training the model 
## maybe lower it 5 epochs 
epochs=5
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)
#0.97


## Ploting the accuarcy 
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()


## Testing on the test dataset 
#1. Load that test csv file into a dataframe
#2. Create a loop, based on that data frame
#3. In the loop, load the file for that row, then run it through the model
#4. Use the argmax function to get the index of the prediction array with the highest probability
import pandas as pd

dat = pd.read_csv("test/test_classes.csv")

predict_name_list = []
per_conf_score_list = []

for i in range(0,12630):
  filename = dat.Filename[i]

  img = keras.preprocessing.image.load_img(
    "test/" + filename, target_size=(100,100))
  
  img_array = keras.preprocessing.image.img_to_array(img)
  img_array = tf.expand_dims(img_array, 0) # Create a batch
  
  predictions = model.predict(img_array)
  score = tf.nn.softmax(predictions[0])
  predict_name_list.append(class_names[np.argmax(score)])
  per_conf_score_list.append((100 * np.max(score)))


  #print(
   #   "This image most likely belongs to {} with a {:.2f} percent confidence."
    #  .format(class_names[np.argmax(score)], 100 * np.max(score))
     # )




## making new cols
dat["Predicted_Name"] = predict_name_list
dat["Conf_Score"] = per_conf_score_list
dat["Predicted_Name"] = dat["Predicted_Name"].astype(int)

correct = []

for i in range(0,12630):

  if dat.ClassId[i] == dat.Predicted_Name[i]:
    correct.append("True")
  else:
    correct.append("False")

dat["Correct"] = correct
dat.Correct.value_counts()



11769/(11769+861)



sns.scatterplot(data=dat, x="ClassId", y="Predicted_Name", alpha= .2)


## Writing the model to Apple's carkit 
get_ipython().getoutput("pip install --upgrade coremltools")
import coremltools

# Save the model
model.save('CarSign.mlmodel')



### Data Augentation -- giving it some flips and rotations  

data_augmentation = keras.Sequential(
  [
    layers.experimental.preprocessing.RandomFlip("horizontal", 
                                                 input_shape=(100, 
                                                              100,
                                                              3)),
    layers.experimental.preprocessing.RandomRotation(0.1),
    layers.experimental.preprocessing.RandomZoom(0.1),
  ]
)
  



plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
  for i in range(9):
    augmented_images = data_augmentation(images)
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_images[0].numpy().astype("uint8"))
    plt.axis("off")


model = models.Sequential([
  data_augmentation,
  layers.experimental.preprocessing.Rescaling(1./255),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Dropout(0.2),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])


model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


epochs = 5
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()


#sunflower_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
#sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

#image_url = "http://1.bp.blogspot.com/-rYWpzb3jYNo/UdQ0M2XUpfI/AAAAAAAAAU0/1bQu8s52aWY/s1600/dscf7299.jpg"
#image_path = tf.keras.utils.get_file("dscf7299", origin = image_url)

## Prefect signs website https://www.rhinocarhire.com/Drive-Smart-Blog/Drive-Smart-Germany/Germany-Road-Signs.aspx

image_url = "https://www.rhinocarhire.com/CorporateSite/media/Drive-Smart/Road-Signs/Prohibitory-Signs/Germany-Prohibitory-Sign-Driving-faster-than-indicated-prohibited-(speed-limit).png"
image_path = tf.keras.utils.get_file("Germany-Prohibitory-Sign-Driving-faster-than-indicated-prohibited-(speed-limit", origin = image_url)

img = keras.preprocessing.image.load_img(
    image_path, target_size=(100,100)
)
img_array = keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)


## Testing on the test dataset 
#1. Load that test csv file into a dataframe
#2. Create a loop, based on that data frame
#3. In the loop, load the file for that row, then run it through the model
#4. Use the argmax function to get the index of the prediction array with the highest probability
import pandas as pd

dat = pd.read_csv("test/test_classes.csv")

predict_name_list = []
per_conf_score_list = []

for i in range(0,12630):
  filename = dat.Filename[i]

  img = keras.preprocessing.image.load_img(
    "test/" + filename, target_size=(100,100))
  
  img_array = keras.preprocessing.image.img_to_array(img)
  img_array = tf.expand_dims(img_array, 0) # Create a batch
  
  predictions = model.predict(img_array)
  score = tf.nn.softmax(predictions[0])
  predict_name_list.append(class_names[np.argmax(score)])
  per_conf_score_list.append((100 * np.max(score)))


  #print(
   #   "This image most likely belongs to {} with a {:.2f} percent confidence."
    #  .format(class_names[np.argmax(score)], 100 * np.max(score))
     # )




dat["Predicted_Name"] = predict_name_list
dat["Conf_Score"] = per_conf_score_list
dat["Predicted_Name"] = dat["Predicted_Name"].astype(int)


correct = []

for i in range(0,12630):

  if dat.ClassId[i] == dat.Predicted_Name[i]:
    correct.append("True")
  else:
    correct.append("False")




dat["Correct"] = correct
dat.head()


dat.info()


11729/12630


sns.displot(dat, x="Conf_Score", binwidth = 10)


sns.scatterplot(data=dat, x="ClassId", y="Predicted_Name", alpha= .2)
