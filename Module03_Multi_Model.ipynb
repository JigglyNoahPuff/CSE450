{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6290  995]\n",
      " [2644 4641]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.86      0.78      7285\n",
      "           1       0.82      0.64      0.72      7285\n",
      "\n",
      "    accuracy                           0.75     14570\n",
      "   macro avg       0.76      0.75      0.75     14570\n",
      "weighted avg       0.76      0.75      0.75     14570\n",
      "\n",
      "[[6281 1035]\n",
      " [2918 4398]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.86      0.76      7316\n",
      "           1       0.81      0.60      0.69      7316\n",
      "\n",
      "    accuracy                           0.73     14632\n",
      "   macro avg       0.75      0.73      0.73     14632\n",
      "weighted avg       0.75      0.73      0.73     14632\n",
      "\n",
      "[[6349  995]\n",
      " [2495 4849]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.86      0.78      7344\n",
      "           1       0.83      0.66      0.74      7344\n",
      "\n",
      "    accuracy                           0.76     14688\n",
      "   macro avg       0.77      0.76      0.76     14688\n",
      "weighted avg       0.77      0.76      0.76     14688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "## Import for partitioning the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Classifier Imports\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Imports for showing the data\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "# Import for saving the model\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "### Reading in the data##################################################################################\n",
    "dat = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bank.csv\")\n",
    "#########################################################################################################\n",
    "\n",
    "### Encoding the data ###################################################################################\n",
    "### Assigning number values to all strings and removing 'contact' and 'day_of_week'\n",
    "## Seperating numerical and categorical so numerical does not get encoded\n",
    "dat_cat = dat.drop([\"age\", \"campaign\", \"pdays\",\t\"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\", 'contact', 'day_of_week'], axis = 1)\n",
    "dat_num = dat.drop([\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"day_of_week\", \"poutcome\", \"y\"], axis = 1)\n",
    "\n",
    "# Rejoining numerical and categorical\n",
    "dat_encoded = dat_cat.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "dat_encoded = pd.concat([dat_num, dat_encoded], axis=1)\n",
    "##########################################################################################################\n",
    "\n",
    "#####Creating the High and Low Economy####################################################################\n",
    "dat_high = dat_encoded[ dat_encoded[\"cons.conf.idx\"] >= -40]\n",
    "dat_low = dat_encoded[ dat_encoded[\"cons.conf.idx\"] < -40]\n",
    "##########################################################################################################\n",
    "\n",
    "### Creating the features and the target##################################################################\n",
    "X = dat_encoded.drop('y', axis=1)\n",
    "y = dat_encoded['y']\n",
    "\n",
    "X_high = dat_high.drop('y', axis=1)\n",
    "y_high = dat_high['y']\n",
    "\n",
    "X_low = dat_low.drop('y', axis=1)\n",
    "y_low = dat_low['y']\n",
    "\n",
    "## Partitioning the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "X_train_high, X_test_high, y_train_high, y_test_high = train_test_split(X, y, test_size=0.20)\n",
    "X_train_low, X_test_low, y_train_low, y_test_low = train_test_split(X, y, test_size=0.20)\n",
    "###########################################################################################################\n",
    "\n",
    "####OVERSAMPLING###########################################################################################\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# https://imbalanced-learn.readthedocs.io/en/stable/user_guide.html\n",
    "\n",
    "\n",
    "# Let's over sample the minority class, which samples with replacement until the\n",
    "# majority (died) and the minority (survived) are equal\n",
    "ro = RandomOverSampler()\n",
    "\n",
    "# Oversample, note that we oversample X and y at the same time in order to \n",
    "# make sure our features and targets stay synched.\n",
    "X_train_new, y_train_new = ro.fit_resample(X_train, y_train)\n",
    "X_test_new, y_test_new = ro.fit_resample(X_test, y_test)\n",
    "\n",
    "X_train_new_high, y_train_new_high = ro.fit_resample(X_train_high, y_train_high)\n",
    "X_test_new_high, y_test_new_high = ro.fit_resample(X_test_high, y_test_high)\n",
    "\n",
    "X_train_new_low, y_train_new_low = ro.fit_resample(X_train_low, y_train_low)\n",
    "X_test_new_low, y_test_new_low = ro.fit_resample(X_test_low, y_test_low)\n",
    "############################################################################################################\n",
    "\n",
    "#### Three different types of classifiers that were tried###################################################\n",
    "\n",
    "### OG Classifier\n",
    "### Was nice, but only can do so much\n",
    "#classifier = DecisionTreeClassifier()\n",
    "#classifier.fit(X_train_new, y_train_new)\n",
    "\n",
    "\n",
    "### Forest Classifier\n",
    "### A forest of decision trees.\n",
    "### Best precision of the three at ~0.86, but recall was low at ~0.38\n",
    "#clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=0)\n",
    "#clf = clf.fit(X_train_new, y_train_new)\n",
    "\n",
    "##Bagging Classifier\n",
    "# Personal Favorite and makes the most sense to use because of how deep our decision tree is.\n",
    "# Best Recall at ~0.64 and precision at ~0.80\n",
    "clf = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.25)\n",
    "clf = clf.fit(X_train_new, y_train_new)\n",
    "\n",
    "clf_high = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.25)\n",
    "clf_high = clf.fit(X_train_new_high, y_train_new_high)\n",
    "\n",
    "clf_low = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.25)\n",
    "clf_low = clf_low.fit(X_train_new_low, y_train_new_low)\n",
    "#############################################################################################################\n",
    "\n",
    "####Saving the Model#########################################################################################\n",
    "dump(clf, 'teddyjustrightModel.joblib')\n",
    "dump(clf_high, 'teddy2highModel.joblib')\n",
    "dump(clf_low, 'teddy2lowModel.joblib') \n",
    "#############################################################################################################\n",
    "\n",
    "##PREDICITON TIME############################################################################################\n",
    "y_pred = clf.predict(X_test_new)\n",
    "\n",
    "y_pred_high = clf_high.predict(X_test_new_high)\n",
    "\n",
    "y_pred_low = clf_low.predict(X_test_new_low)\n",
    "\n",
    "## Displaying the data\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test_new, y_pred))\n",
    "print(classification_report(y_test_new, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test_new_high, y_pred_high))\n",
    "print(classification_report(y_test_new_high, y_pred_high))\n",
    "\n",
    "print(confusion_matrix(y_test_new_low, y_pred_low))\n",
    "print(classification_report(y_test_new_low, y_pred_low))\n",
    "\n",
    "\n",
    "### Showing the tree, but it takes a hot minute cause the tree is so big\n",
    "#fig, ax = plt.subplots(figsize=(20, 20))\n",
    "#tree.plot_tree(classifier, fontsize=10, feature_names=X.columns)\n",
    "#plt.show()\n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
